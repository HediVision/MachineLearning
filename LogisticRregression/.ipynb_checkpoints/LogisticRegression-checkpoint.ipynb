{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "For this example I am using TensorFlow library and MNIST database of handwritten digits.\n",
    "\n",
    "Logistic regression algorithm finds a linear function that predict class $(y)$ given an input vector $x$ as,\n",
    "\n",
    "$$\n",
    "\\hat{y}=\\sum_{n_x}^{j=0} w_jx_j+b=w^Tx+b\n",
    "$$\n",
    "where $X$ is the explanatory variable and $Y$ is the dependent variable. The slope of the line is w, and b is the intercept. Typicaly, the output should be in probability form (0<yÌ‚ <1), and thefore sigmoid function ( or similar function) is applied to predicted output.\n",
    "\n",
    "$$\\hat{y}=P(y=1|x) \\longrightarrow \\hat{y}=\\sigma(w^Tx+b)$$\n",
    "\n",
    "The final goal  here is to find optimal weights $ w $ that best approximate the output y by minimizing \n",
    "the prediction error. The simple loss function or error function is one half of square error as, \n",
    "$$\n",
    "L(\\hat{y},y)=\\dfrac{1}{2}(\\hat{y}-y)^2\\approx 0\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Firts download MNIST dataset from [here](http://yann.lecun.com/exdb/mnist/). Tensorflow allows us to easily load the MNIST data.  If you are using different data you might need to write your own pipeline. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4833afe7bd9f>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "# In one_hot the class-numbers is converted from a single integer to a vector whose length equals \n",
    "#the number of possible classes. For zero the one_hot look like this ==> [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST/\", one_hot=True)\n",
    "train_x = mnist.train.images\n",
    "test_x = mnist.test.images\n",
    "print(test_x.shape[1])\n",
    "fv_size=train_x.shape[1]#size of feature vector\n",
    "num_cls=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "# graph input\n",
    "x     = tf.placeholder(tf.float32,[None, fv_size])\n",
    "y     = tf.placeholder(tf.float32,[None,num_cls])\n",
    "y_cls = tf.placeholder(tf.int64, [None])\n",
    "# Set model weights\n",
    "w     = tf.Variable(tf.zeros([fv_size, num_cls]))# Weights\n",
    "b     = tf.Variable(tf.zeros([num_cls])) #Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model \n",
    "Here I used  Softmax Regression which is a generalization of logistic regression that we can use for multi-class classification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.nn.softmax(tf.matmul(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize error\n",
    "The half a square error might cause the optimization problem (non-convex problem). To prevent multiple local optima the loss function can be written as,  \n",
    "$$\n",
    "L(\\hat{y},y)=-\\big(y\\log \\hat{y}+(1-y)\\log(1-\\hat{y}) \\big)\n",
    "$$\n",
    "Above equation measures error for one a single training example. The overall performance over $ m $ training  is indicated by cost function $ J $ as,  \n",
    "$$\n",
    "J(w,b)=\\dfrac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^i,y^i)\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_hat), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'cost=', '1.183184105')\n",
      "('Epoch:', '0002', 'cost=', '0.665269974')\n",
      "('Epoch:', '0003', 'cost=', '0.552796931')\n",
      "('Epoch:', '0004', 'cost=', '0.498691439')\n",
      "('Epoch:', '0005', 'cost=', '0.465480853')\n",
      "('Epoch:', '0006', 'cost=', '0.442601104')\n",
      "('Epoch:', '0007', 'cost=', '0.425521235')\n",
      "('Epoch:', '0008', 'cost=', '0.412171783')\n",
      "('Epoch:', '0009', 'cost=', '0.401396890')\n",
      "('Epoch:', '0010', 'cost=', '0.392414389')\n",
      "('Epoch:', '0011', 'cost=', '0.384699149')\n",
      "('Epoch:', '0012', 'cost=', '0.378190482')\n",
      "('Epoch:', '0013', 'cost=', '0.372417551')\n",
      "('Epoch:', '0014', 'cost=', '0.367313159')\n",
      "('Epoch:', '0015', 'cost=', '0.362695270')\n",
      "('Epoch:', '0016', 'cost=', '0.358600007')\n",
      "('Epoch:', '0017', 'cost=', '0.354862887')\n",
      "('Epoch:', '0018', 'cost=', '0.351458699')\n",
      "('Epoch:', '0019', 'cost=', '0.348322929')\n",
      "('Epoch:', '0020', 'cost=', '0.345416409')\n",
      "('Epoch:', '0021', 'cost=', '0.342750701')\n",
      "('Epoch:', '0022', 'cost=', '0.340278062')\n",
      "('Epoch:', '0023', 'cost=', '0.337937795')\n",
      "('Epoch:', '0024', 'cost=', '0.335765422')\n",
      "('Epoch:', '0025', 'cost=', '0.333681539')\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "for epch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _,c = sess.run([optimizer,J], feed_dict={x: batch_x,y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        if (epch+1) % 1 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.9144)\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
